%------------------------------------------------------------------------------
% Template file for the submission of papers to IUCr journals in LaTeX2e
% using the iucr document class
% Copyright 1999-2013 International Union of Crystallography
% Version 1.6 (28 March 2013)
%------------------------------------------------------------------------------

\documentclass[pdf]{iucr}              % DO NOT DELETE THIS LINE
\usepackage{graphicx}
\usepackage{color}
\usepackage{booktabs}
     %-------------------------------------------------------------------------
     % Information about journal to which submitted
     %-------------------------------------------------------------------------
     \journalcode{S}              % Indicate the journal to which submitted
                                  %   A - Acta Crystallographica Section A
                                  %   B - Acta Crystallographica Section B
                                  %   C - Acta Crystallographica Section C
                                  %   D - Acta Crystallographica Section D
                                  %   E - Acta Crystallographica Section E
                                  %   F - Acta Crystallographica Section F
                                  %   J - Journal of Applied Crystallography
                                  %   M - IUCrJ
                                  %   S - Journal of Synchrotron Radiation

\begin{document}                  % DO NOT DELETE THIS LINE

     %-------------------------------------------------------------------------
     % The introductory (header) part of the paper
     %-------------------------------------------------------------------------

     % The title of the paper. Use \shorttitle to indicate an abbreviated title
     % for use in running heads (you will need to uncomment it).

\title{Scientific Data Exchange: a schema for HDF5-based storage of raw and analyzed data}
%\shorttitle{Short Title}

     % Authors' names and addresses. Use \cauthor for the main (contact) author.
     % Use \author for all other authors. Use \aff for authors' affiliations.
     % Use lower-case letters in square brackets to link authors to their
     % affiliations; if there is only one affiliation address, remove the [a].

\cauthor[a]{Francesco}{De Carlo}{decarlo@aps.anl.gov}{} 
\author[a]{Do\v{g}a}{G\"{u}rsoy}
\author[b]{Federica}{Marone} 
\author[c]{Mark}{Rivers} 
\author[f]{Dilworth Y.}{Parkinson}
\author[a]{Faisal}{Khan} 
\author[a]{Nicholas}{Schwarz} 
\author[a]{David J.}{Vine}
\author[a]{Stefan}{Vogt}
\author[a]{Sophie-Charlotte}{Gleber}
\author[a]{Suresh}{Narayanan}
\author[c]{Matt}{Newville}
\author[c]{Tony}{Lanzirotti}
\author[d]{Yue}{Sun}
\author[d]{Young Pyo}{Hong}
\author[a,d,e]{Chris}{Jacobsen}


\aff[a]{Advanced Photon Source, Argonne National Laboratory, 9700 S.~Cass Ave., Argonne IL 60439 \country{USA}}
\aff[b]{Swiss Light Source, Paul Scherrer Institut, Villigen \country{Switzerland}}
\aff[c]{The University of Chicago, Center for Advanced Radiation Sources, Argonne National Laboratory, 9700
  S.~Cass Ave., Argonne IL 60439 \country{USA}}
\aff[d]{Dept.~Physics \&{} Astronomy, Northwestern University, 2145
  Sheridan Road, Evanston IL 60208 \country{USA}}
\aff[e]{Chemistry of Life Processes Institute, Northwestern University, 2170 Campus Drive, Evanston, IL 60208 \country{USA}}
\aff[f]{Advanced Light Source, 6 Cyclotron Road, Berkeley, CA 94720, \country{USA}}


     % Keywords (required for Journal of Synchrotron Radiation only)
     % Use the \keyword macro for each word or phrase, e.g. 
     % \keyword{X-ray diffraction}\keyword{muscle}

\keyword{data storage, provenance}
\keyword{HDF5}

\hyphenpenalty=10000

\maketitle                        % DO NOT DELETE THIS LINE

\begin{synopsis}
Supply a synopsis of the paper for inclusion in the Table of Contents.
\end{synopsis}

\begin{abstract}
Data Exchange is a simple data model designed to interface, or ``exchange'' data among different instruments, and to enable sharing of data analysis tools. Data Exchange focuses on technique rather than instrument descriptions, and on provenance tracking of analysis steps and results. In this paper we describe the successful application of the Data Exchange model to a variety of X-ray techniques, including tomography, fluorescence spectroscopy, fluorescence tomography and photon correlation spectroscopy.
\end{abstract}


\section{Introduction}

When it comes to the digital storage of experimental data and analyses results at synchrotron light sources around the world, the situation resembles Babel \cite{genesis}. As different research teams and techniques have grown at various facilities, they have often developed local data storage formats based on instrument hardware specificity and expediency rather than rational planning, often drawing upon the particular preferences of a scientist or engineer writing software at the project's outset. In some cases, simple text files are used because of their human readability in spite of their inefficiency with respect to storage size and the cost of parsing text. In other cases, images are stored using common image files, but without systematically saving the metadata describing experiment conditions or analysis parameters.

The Data Exchange is a simple data model that is designed to interface, or ``exchange'' data among different instruments, and to enable sharing of data analysis tools. This is particularly important since more and more scientific users perform experiments at different synchrotron light sources \cite{Kanitpanyacharoen}. The Data Exchange implementation uses the Hierarchical Data Format 5 or HDF5 \cite{HDF5}, a widely used and supported storage format for scientific data. The Data Exchange is highly simplified and focuses on technique rather than instrument descriptions, and on provenance tracking for understanding analysis steps and results. Provenance information is stored in a manner that can be used with a workflow pipeline to automatically run analysis steps while maintaining human readability.

Here we describe the successful application of the Data Exchange model to a variety of synchrotron-based techniques, including X-ray tomography, X-ray fluorescence spectroscopy, X-ray fluorescence tomography, coherent diffraction imaging and X-ray photon correlation spectroscopy.

\section{Background}

A popular image format used in the scientific community is the Tagged Image File Format (TIFF). The TIFF standard allows for the addition of ``private'' tags beyond those used by generally available software. This feature has been used by the Open Microscopy Environment's data format \cite{OME-TIFF} and the GeoTIFF format \cite{GeoTIFF}. However, TIFF files suffer from file size limitations, and may require the use of separate metadata files to describe data that doesn't logically fit into a series of separate image files.

The Crystallographic Information File (CIF) is a successful example of a standard file format for representing crystallographic data. In addition, the Protein Data Bank uses the pdb format \cite{pdb} to store three-dimensional molecular structures in a standardized way. Both formats store data in standard text files, which is adequate due to the relatively small size of these files with respect to imaging datasets.

Within the synchrotron light source and neutron facility community, NeXuS \cite{Tischler1984} has a long history of development, though its adoption at synchrotron light sources in particular has been uneven and its design is centered primarily on the storage of as-acquired data \cite{NeXuS}. New European initiatives like the Photon and Neutron data infrastructure \cite{PanData_2013}, and the High Data Rate Initiative for Photon, Neutrons and Ions \cite{PNI_HDRI_2013} are
planning to extend the NeXuS interfaces between various instrument controls and NeXuS libraries creating NeXuS definitions to fully describe the instruments where the data were collected. 
For X-ray fluorescence microprobes (XFM) no distinct file format exists, even though basic data structure is similar among beamlines. Most XFM beamlines operating in an imaging mode typically save full energy dispersive spectra as a data array consisting of energy bins vs. intensity (either as a single spectra per pixel or as a list-mode event stream) along with pixel (or motor) coordinates, scaler information, etc.Yet despite this commonality the situation today is that by and large every beamline saves data in a different format, making the development of a common set of data processing tools difficult.

The HDF5 library is a widely used and supported open-source binary file format and supporting utilities for large scientific datasets available for use in a myriad of programming languages \cite{HDF5}. HDF5 files are self-describing and portable, which anyone who has tried to decode an undocumented binary data file can immediately appreciate. HDF5 files are also used in the financial services industry \cite{Bethel_2011}, and its advantages have been appreciated for biological imaging \cite{dougherty_cacm_2009,eliceiri_nm_2012}, for X-ray spectroscopy
\cite{ravel_jsr_2012,medjoubi_jsr_2013}, for X-ray fluorescence microscopy analysis packages \cite{vogt_jdp_2003,sole_sab_2007}, and for coherent diffraction imaging \cite{steinbrener_oe_2010,maia_nm_2012}. While the NeXuS format noted above allows for implementation via XML and HDF4 files as well as HDF5 files, most new implementations of NeXuS are concentrating on using the HDF5 file format.


\section{The Data Exchange Model}

Because of the impossibility of designing a schema that incorporates all possible types of data collection, analysis results, and data representations, the Data Exchange definition limits the amount of required structures, while allowing correct Data Exchange files to include as much additional information as desired.  

The key principle of Data Exchange is that, in most cases, for each experiment technique, there is one particular data array that an analysis or visualization program, or researcher will want to access. For example, this may be a series of normalized projections, a spectrum, a spectrum image, a scattering pattern, or a reconstructed 3D volume. This key array is placed into an HDF5 group called \texttt{exchange} for easy identification.

This gives maximum flexibility for various analysis programs to add various derived results to an original data file without compromising or altering the section of the file devoted to original storage of the acquired data. Instrument and data collection metadata tags are given fixed names with a naming scheme meant to be maximally compatible with both NeXuS \cite{NeXuS} and CXI \cite{maia_nm_2012} files, and are meant to be descriptive enough for easy human readability using an application such as \texttt{HDFView}. 

Many scientists have probably seen data files that refer to a wavelength without specifying whether is in \AA{}ngstroms or inverse centimeters, or that refer to an angle without indicating whether is in degrees or microradians. In Data Exchange, all variables are required to specify the physical units. The text strings that describe the units should conform to those defined by the UDUNITS package \cite{ucar_2013}.

Finally, Data Exchange uses plain HDF5 calls, rather than a separate Application Programming Interface (API). In this way one avoids the rather substantial effort that would otherwise be required to debug and maintain an API across many platforms for an ever changing set of requirements.

Again, our goal is that one should maximize both the future extensibility of a file to meet evolving data acquisition schemes and data analysis tools, and also human readability (via e.g. \texttt{h5dump} or \texttt{HDFView}) so that one can manually examine a file without access to the computer code used to write it.

\subsection{Storing Data}

The full definition for Data Exchange HDF5 files is published online \cite{data_exchange}. Here we outline the key characteristics of a basic Data Exchange file, with the understanding that \cite{data_exchange} contains the reference description as well as a number of example Data Exchange read and write programs in several computer languages. A simple Data Exchange file layout is shown in Table \ref{tab:genrules}.

While HDF5 gives great flexibility in data storage, straightforward file readability and exchange requires adhering to an agreed-upon naming and organizational convention. To achieve this goal, Data Exchange adopts a layered approach by defining a set of \emph{mandatory} and \texttt{optional} fields. That is, even a simple Data Exchange file should be able to be considered ``correct'' according to the schema, while allowing for as rich a set of metadata tags as any technique might desire.  

In a minimal Data Exchange file, the only mandatory items are a HDF5 group called \emph{exchange} and a HDF5 dataset called \emph{implements}. The \emph{implements} dataset is a string that describes which ``exchange'' groups have been added to the file. This expedient obviates the need to parse the file structure to see if a particular group exists. As mentioned previously, \emph{exchange} is the data of primary interest in the file.  The \emph{exchange} group may contain any number of HDF5 datasets that contain actual data or links to data points or arrays. Associated values that relate to the data in the \emph{exchange} group, and that may be considered axis values for the data should be stored in additional datasets in the \emph{exchange} group. This data may be the times and angles at which projections were taken, for example. Data Exchange uses the dimension scales HDF5 feature (an HDF5 feature that is incidentally not presently supported by NeXuS) to associate the array of data values with its associated axis information. In part because of this requirement, Data Exchange requires use of HDF5 version 1.8 or later, all other software requirements are listed at \cite{data_exchange}. Multiple \emph{exchange} groups can be added to a file by appending a ``\texttt{\_N}" to the name of the group, for example, \emph{exchange\_4}.

A more general Data Exchange file might also contain the optional \texttt{measurement} group. This group contains sample and instrument information which is expected to be static throughout the measurement (e.g., sample preparation information, and instrument configuration). As with the \emph{exchange} group, multiple \texttt{measurement} groups can be added to a file by appending a ``\texttt{\_N}" to the group name, for example, \texttt{measurement\_4}.

\begin{table}
\centering
\footnotesize
\caption{A Simple Data Exchange File}
\label{tab:genrules}
\begin{tabular}{l l l}
\toprule
\bfseries HDF5 Object & \bfseries Description \\
\midrule
\emph{exchange} & HDF5 Group for primary data \\
\hspace{2pt} \tt{projections} & A 3D array of image projections taken over time \\
\hspace{2pt} \tt{time} & A 1D array of time stamps for projections \\
\hspace{2pt} \tt{theta} & A 1D array of angles corresponding to projections  \\
\tt{measurement} & HDF5  Group \\
\hspace{2pt} \tt{instrument} & HDF5 Group for instrument definitions \\
\hspace{2pt} \tt{sample} & HDF5 Group for static sample information \\
\tt{provenance} & HDF5 Group for provenance information \\
\hspace{2pt} \tt{process\_table} & Table storing list of provenance processes \\
\hspace{2pt} \tt{copy} & Group with parameters for a copy operation \\
\hspace{2pt} \tt{reconstruction} & Group with parameters for reconstruction \\
\bottomrule
\end{tabular}
\end{table}


Beyond this, additional groups may be added to meet individual needs, with guidelines suggesting the best structure.


\subsection{Maintaining Provenance}

Another optional group is the \texttt{provenance} group. This group contains information about all transformations, analyses and interpretations of data performed by a sequence of process functions. Maintaining this history allows for reproducible representations of data. The Data Exchange format tracks provenance with a provenance process table. The provenance process table tracks the execution order of a series of processes via a list of sequential entries in the process table. The Data Exchange model uses this approach instead of using a separate traditional relational database to maintain provenance information for two main reasons. One is so that all relevant information regarding actions taken on the data may be stored together along with the data in one container. The other is that standard relational databases require stricter up-front definitions of tables and data types which requires more effort when extending as opposed to adding more key-value pairs.

An example of the provenance process table is shown in Table ~\ref{table:provenance}. Rows in the table represent actions performed on the data. Each row has a number of properties associated with it, some of which are name, status, message, and reference. Other properties are omitted for the sake of brevity.

\begin{table}
\centering
\footnotesize
\caption{An Example Provenance Process Table}
\label{table:provenance}
\begin{tabular}{l l l l l l l l l }
\toprule
\bfseries Name & \bfseries Status & \bfseries Message & \bfseries Reference \\
\midrule
copy & FAILED & auth. error & /provenance/copy  \\
copy & SUCCESS & OK & /provenance/copy  \\
norm & SUCCESS & OK & /provenance/norm  \\
reconstruction & SUCCESS & OK & /provenance/reconstruction  \\
convert & RUNNING & & /provenance/export  \\
copy & QUEUED & & /provenance/copy\_2 \\
\bottomrule
\end{tabular}
\end{table}

The most important property in the process table is the reference. The reference is a text string that refers to another group in the HDF5 file that describes the parameters needed to perform a particular process on the data. For something simple like a file transfer, the group may contain source and destination Uniform Resource Identifiers (URIs). For an analysis step, the group may contain parameters for the analysis algorithm, including the input and output data sets within the file used for that specific instance of a process. Increasing the parameters within the appropriate reference group allows for the representation of arbitrarily complex analysis processes. Likewise, increasingly complex workflows of many analysis steps may be constructed by adding more entries to the process table. Workflow pipelines, such as mentioned in the use cases below, may use this information to rerun analysis attempts due to either a failure or a desire to modify parameters resulting in additional entries in the table, and possibly other parts of the file. In order to maintain human understandability, it's encouraged to make parameters for analysis steps the actual algorithm parameters and not only the text of the command line arguments required to run a specific tool.

Scientific users are not generally expected to maintain data in this group, however. The expectation is that analysis pipeline tools will automatically modify processing steps using this group. An analysis workflow pipeline compatible with Data Exchange files has been demonstrated for X-ray tomography experiments in \cite{schwarz_icalepcs_2013}, and for X-ray photon correlation spectroscopy analysis in \cite{khan_icalepcs_2013}.

\section{Data Exchange for Full--Field X--ray Tomography}

A tomographic raw data set consists of a series of projections (data), dark field (data\_dark), and white field (data\_white) images. Since dark and white fields can be collected at any time before, after or during the projection data collection, Data Exchange uses the angular position of the tomographic rotation axis, theta, to keep track of when the dark and white images were collected. Data Exchange saves the raw dataset in 3D arrays using, by default, the natural HDF5 order of a multidimensional array (rotation axis, ccd y, ccd x), i.e.\  with the fastest changing dimension being the last dimension, and the slowest changing dimension being the first dimension. The definition of the Data Exchange implementation for tomography, see \cite{data_exchange}, allows for storing of intermediate processing steps like normalized projections. Normalized projections represent the first data entry array for any subsequent 3D reconstruction algorithm.

At the Advanced Photon Source (APS), the areaDetector software \cite{area_detector} is used to integrate various cameras into the APS control system. A specialized areaDetector plug--in under development to write fully compliant Data Exchange HDF5 files. 

The raw data are subsequently processed using tomoPy an open source framework developed at the Advanced Photon Source for the analysis of synchrotron tomographic data \cite{tomoPy}.

At the Swiss Light Source, the need for a new approach for digital storage of experimental data has become evident during the recent developments towards an ultrafast tomography endstation \cite{mokso2010}. The new in-house developed read out system (GIGAbit Fast Read-Out SysTem - GIGAFROST) for a CMOS detector permits the read out of the sensor in a fully continuous unlimited mode achieving rates as high as 8 GB/s. To efficiently handle and in particular post process this large amount of raw data, at rates consistent with the acquisition, the current reconstruction pipeline has been revisited at different levels. A major Input/Output (I/O) bottleneck has clearly been identified in the originally used TIFF format, with thousands small files for individual projections. The HDF5 technology enables significant improvements in the current I/O performance, bringing performance close to hardware limits of modern shared file systems, 8 GB/s for the current General Parallel File System (GPFS) setup. In our implementation of the data collection, individual User Datagram Protocol (UDP) datagrams dispatched by GIGAFROST are assembled to images in shared memory where the data are represented as 3dnumpy arrays, i.e. stacks or series of images in python. When the tomographic dataset is complete, the data is dumped in a sequential way to disk to a HDF5 file using the direct chunk write function \cite{donath2013} and a nbit-filter, for instance, for native 12 bit images. No additional compression algorithms are used for X-ray tomography data, since the achieved compressibility factor is in general very low (less than 1.5).

The Data Exchange model is particularly attractive for its simplicity combined with flexibility and completeness. In the first phase, we focused exclusively on the raw data, in addition to the metadata characterizing the acquisition setup and parameters. In the simplest implementation we store the raw data (projections, dark and white fields) in their respective datasets in the \emph{exchange} group using the natural order of multidimensional arrays in HDF5. The flexibility of the Data Exchange model with its object attributes, permits us to test different array orders to optimize performance, without need for changes in the post processing software. If for instance the default first and second dimensions are swapped, the data are actually directly organized as uncorrected sinograms. This arrangement brings an advantage during post processing, if no projection based pre processing (e.g. standard phase retrieval) is needed. We measured a 35\% acceleration in the generation of corrected sinograms if raw data in default HDF5 - Data Exchange compliant format are used as opposed to individual TIFF files. An improvement as high as 57\% is instead obtained if this alternative organization of the multidimensional arrays is used. Fine tuning of the raw data organization, including different chunking strategies, is currently on-going to reach best I/O performance. Best results seem to require an intermediate chunk size, while small chunks implying a large amount of chunks for our typical datasets are penalizing the performance. The tests presented here have been run on a distributed environment, with multiple processes reading simultaneously. The used file system is GPFS, a high performance parallel file system. Other file systems (e.g. NTFS or tiered file systems) are not suitable for our applications.
The flexibility of the even simplest Data Exchange format has also already been proven in recent tomographic dynamic experiments, where the onset of the process of interest was not easy to control. In such a case it is difficult to obtain two high quality 3D volumes right before and after the changes occurring. To obviate this difficulty, we continuously acquired and stored projections in a well documented single HDF5 file, while rotation of the sample over several revolutions, instead of standard tomographic datasets implying a rotation of 180 degrees. After a-posteriori recognition of the event time point, the post processing software could easily extract the relevant projections for the reconstruction of the volumes of interest. 

In a second phase, we plan also to take advantage of additional features provided by the Data Exchange model. The aspect related to provenance is of particular interest, which we envisage to link to our database as well as graphical user interface, so to easily have a control over the outcome of the different steps of the post processing pipeline. In addition, on going internal discussions are devoted at establishing the optimal internal file organization, both for storing raw data for complicated experiments involving multiple scans (for instance at different conditions), and when needed for intermediate post processing results.

At the Swiss Light Source python has been the main language for implementation of the data backend system including writing of Data Exchange compliant files. C/C++ has been used when the achieved performance was not sufficient as for instance for receiving, from the camera, UDP datagrams and assembling them in shared memory to images. Python has also been the language of choice in combination with the flexible Message Passing Interface (MPI) for data reading and post processing. To improve the computational performance, for some selected parts, Cython has been used.

At the Advanced Light Source (ALS), data rates and volumes are increasing at an unprecedented rate. At the same time, analysis and simulation software for lightsource data are increasingly sophisticated and represent greater investment in development as well as providing greater capability. To make these advanced capabilities available to more scientists, we have developed a suite of data management, processing, analysis and simulation tools named SPOT Suite \cite{Spot_suite}, in collaboration with Lawrence Berkeley Lab's Computational Research Division and the National Energy Research Scientific Computing Center (NERSC). HDF5 was chosen as the data format for SPOT Suite, but during initial developments a non-specialized HDF5 format was used. Because the data processing and analysis needs of the hard x-ray tomography beamline at the ALS are similar to those at other synchrotron tomography beamlines, a common file format optimized for tomographic data could greatly facilitate sharing of and collaboration on processing software, and improves the user experience for those who use multiple different facilities. Initial software is being developed to allow SPOT Suite to begin using Data Exchange for tomography data, and these changes will soon be put into the production version of the code.

\section{Data Exchange for X--ray Fluorescence Microscopy}

An X-ray fluorescence data set typically consists of a series of elemental maps (images), derived from raw X-ray fluorescence spectra acquired with energy dispersive detector systems in a scanning probe geometry. Depending on the specific instrumentation, these may be comprised of single or multiple individual detector elements. These data are typically complemented with scalar per-pixel information, such as incident flux, transmitted flux, additional detector information (live time, count-rate, \ldots). Typically, each of these elemental maps give the planar distribution (sample x vs sample y) of one specific element of interest in the sample studied, although data acquisition strategies that record individual scan lines (1D) or tomographic line projections (sample x vs sample theta) are also encountered. Full XRF tomography is typically acquired in a series of individual (separately saved and processed) projections, and discussed below. We note that intrinsically, there is significant variation in the type and amount of data acquired in scanning microprobes. For example, one system may use just one single element energy dispersive spectroscopy detector counting for a fixed elapsed live time, whereas another system may use a 96 element XRF detector requiring per channel normalization. The goal of the exchange implementation, at minimum, is to make it possible to open the file with a variety of software tools, and work with the data no matter the source, and specific instrument configuration. In addition, we note that the built in support of compression into HDF5 can be a significant boon in particular for XRF microprobes. Significant savings in file size can typically be achieved on fluorescence spectra. 

The primary data produced by an X-ray fluorescence microscope is a number of photons detected with a given energy originating from a particular spatial location on the sample. This raw data can then be analyzed in a number of ways to produce the (quantified) spatial distribution of elemental concentration that is typically the goal of the measurement. Data Exchange for X-ray fluorescence microscopy strives to make the raw data available for (re-)analysis with the knowledge that most end users and visualization software will work with analyzed data. To this end we reserve ``exchange\_0'' for the raw data and ``exchange\_\{1,2,\ldots,N\}'' for the analyzed data. The ``exchange'' group is designated as a soft link to the dataset most relevant to the recipient of the Data Exchange file. In this way we preserve the flexibility of the Data Exchange model and enabling immediate access to the raw data without the need to define special tags which necessitate parsing and searching the entire file for them.

To accommodate arbitrary scan types we have adopted list-based storage of the raw data. In the most general case each pixel of a scan consists of an energy measurement recorded as a function of an independent variable (position, temperature, angle etc.). In the Data Exchange file each measurement is stored in the ``data'' dataset of the ``exchange\_0'' group with an attribute describing the independent axis as, e.g., ``theta:y:x.'' A separate dataset ``axes'' in the ``exchange\_0'' group records the independent variable for each entry in ``data.'' For the most common scan types like a two-dimensional image this approach requires additional processing to arrange the data into an array however it has the advantage of being able to handle a range of scan geometries: Cartesian \& spiral, raster \& zig-zag, one-dimensional and up. Each independent channel of multi-element energy-resolving detectors are stored individually as ``data\_N'' for $N=0,1,\ldots,M$ and the integrated spectra stored as ``data.''

Per-pixel scalar channels (storage ring current, ion chamber, etc) are stored as a list-based dataset ``channels'' in the raw data ``exchange\_0'' group. For $N$ scalar channels each entry in the ``channels'' dataset will be an $N$-element array and the corresponding labels for each channel stored in a separate ``channel\_names'' dataset under ``exchange\_0.'' Optional recommended channel names for standard scalar quantities like the storage ring current are defined in the Data Exchange reference document. Standardizing the channel names greatly aids in the development of automated software by making the data facility and instrument agnostic. 

List-based storage of data is optional for analyzed datasets. In most cases visualization or tomographic reconstruction software will expect data to be arranged in a two- or three-dimensional array and in these cases it makes sense to store the data natively in a Cartesian array of the appropriate dimensionality. For higher dimensional scans (e.g tomograms as a function of time) the list-based approach is recommended.

The analyzed data can be normalized in different ways or not at all. We recommend that each analyzed data exchange group contain the un-normalized data and optionally a normalized dataset. This approach allows easy re-normalization and also accommodates storing a fully analyzed dataset. The normalization method can be described in the attribute of the normalized dataset and the units attribute set accordingly.

As an example of the utility of the Data Exchange model consider X-ray fluorescence tomography. Adopting the Data Exchange format enabled APS scanning probe beamlines to easily make use of the sophisticated and actively developed tomography analysis software created by the more mature full--field tomography beamlines. A data analysis pipeline was quickly adjusted to also handle XRF tomography data. Centralising the tomography reconstruction software development effort reduces duplication of effort and ensures that all tomography users benefit from the continual improvement to the software.

\section{Data Exchange for X--ray Photon Correlation Spectroscopy}

X-ray Photon Correlation Spectroscopy (XPCS) is a unique technique to probe the equilibrium and non-equilibrium dynamics in materials over a wide range of time scales and length scales down to nanometers. A typical XPCS data consists of a time series of 2-D detector images acquired at a constant time interval. By time correlation of the speckle pattern arising due to the scattering of a fully or a partially coherent beam from a disordered material, the wave vector or length scale dependent dynamical time scales of the system being probed can be extracted. During the last decade, XPCS has been successfully applied to probe a wide range of soft and hard matter systems.

Data exchange model has been implemented for XPCS using the following HDF5 groups in conjunction with a high performance computing (HPC) based data analysis system.

\begin{itemize}
\item measurement - comprising of instrument, acquisition, detector and source sub-groups.
\item sample - stores sample related information such as thickness, temperature and other relevant parameters.
\item provenance - containing steps pertaining to the data processing steps.
\item xpcs - a rich set of meta data that contains information regarding how the data needs to be processed in a HPC environment.
	\begin{itemize}
	\item Region of interest of the data to be processed.
	\item Pixellated wave vector map dividing the 2-D array into static and dynamic wave vector maps based on which the individual pixel correlations will be binned to yield the final wave vector dependent correlation functions.
	\item Details of the starting and the end frames to be processed in the case when only a subset of the data is processed.
	\end{itemize}

\item exchange - Contains the following detailed computational results saved in several data sets:
	\begin{itemize}
		\item Wave vector dependent correlation functions computed from the raw data.
		\item Modeled correlation functions based on non-linear least square fitting to exponential and stretched/compressed exponential functions.
		\item Dynamical wave vector dependent time scales extracted from curve fitting.
	\end{itemize}
\end{itemize}

Similar to the implementations for the other techniques presented earlier, multiple analysis performed on the same data set under different processing steps are tracked in a provenance table.

The APS had pioneered the development of XPCS technique using area detectors and has likewise established the lead in developing HPC based data reduction and analysis yielding time auto correlations in near real-time. The successful implementation of the data exchange model to XPCS will enable potential collaboration with the recently built XPCS beamline P-10 at PETRA-III and the upcoming hard X--ray coherent scattering beamline at NSLS-II. 

\section{The future of the Data Exchange Model}

The initial goal of Data Exchange is to facilitate exchange of data and analysis software tools among facilities performing the same or similar technique. The model has been applied to data from a range of facilities including ALS, APS, Anka, Australian Synchrotron facility, Diamond, ESRF ID-19, Elettra, Petra III, SLAC, SLS and X--radia systems. For example, it has already resulted in demonstrable improvements at APS where the tomoPy software developed by the full--field tomography beamlines has been shared to the scanning probe beamlines. We envision that adoption of Data Exchange will empower facility users to select the best analysis tool for their data irrespective of where the data was acquired or where the software was developed.

The Data Exchange strategy is not to solve the ``Babel situation'' described in the introduction by creating yet another format but to create an intermediary that everyone can export to and read from. For this reason, even within the same technique, Data Exchange does not impose rigid definition for instrument components nor of saving instrument status at each data collection point (unlike the NeXus implementation, for example) instead its goal is in finding consensus within each class of instrument in what is the most meaningful and basic raw data set to share.

To expand the Data Exchange model and possibly turn it into the native data format of choice at large facilities we adopted the following strategy:

\begin{itemize}
    \item maintain, expand and freely distribute tomoPy \cite{tomoPy}, the APS tomography reconstruction software that natively uses Data Exchange.
\item expand the ability of tomPy to natively read both Data Exchange and facility-specific data. These data importers are published and constantly updated in the demo section of \cite{data_exchange}.
\item develope a data exchange plug--in for area detector \cite{area_detector} making it possible for all facility using this software to control their camera and to save tomographic data directly in Data Exchange.
\end{itemize}


With more and more users doing experiments at different synchrotron facilities we expect the need to exchange data and software tools will grow and facilitate this approach.
 

\section{Conclusions}

The definition of Data Exchange as a simple data model designed to interface, or ``exchange'' data among different instruments has the potential to improve the interoperability of software toolboxes currently under development at various synchrotron facilities. Perhaps most importantly, the migration to a more standardized file format among beamlines for the techniques described here will empower users. It can be argued that users can then more freely utilize a broader suite of analysis tools that may have been developed at different facilities for different beamlines, but may be best suited to their particular scientific study. This would allow, for example, not only the interfacing of varying data types that are collected simultaneously, but also the integration of complementarity data that may have been acquired at different beamlines (eg, TXM and XRFT) or with non-synchrotron based instruments (optical microscopy, electron beam instruments, etc.). As examples, consider that this would also allow for more seamless registration of 3D image tomographic datasets or for X-ray microfluorescence and microdiffraction data, one collected using energy dispersive detectors and the other using area detectors. For example, at the APS tomoPy \cite{tomoPy} in connection with Data Exchange is being developed to provide the ability to analyze tomography data from all major synchrotron facilities around the world, for various tomographic techniques.



%\appendix
%\section{Appendix title}

%Appendix text.

     % Acknowledgements come after the appendices

\ack{Acknowledgements}

The authors would like to thank Heiner Billich and Alain Studer for the development, implementation and optimization of the data backend system for the Swiss Light Source new ultrafast detector, Rajmund Mokso for leading the GIGAFROST project, Waruntorn (Jane) Kanitpanyacharoen and Hans-Rudolf Wenk for stimulating discussions leading to the round-robin project \cite{Kanitpanyacharoen}, John Hammonds and Timothy Madden for starting the development of an HDF5 plug-in for area detector  and Claude Saunders and Deborah Quock for their effort and support.

Work supported by U.S. Department of Energy, Office of Science, under Contract No. DE-AC02-06CH11357. 

     % References are at the end of the document, between \begin{references}
     % and \end{references} tags. Each reference is in a \reference entry.

\referencelist[dataexchange]

     %-------------------------------------------------------------------------
     % TABLES AND FIGURES SHOULD BE INSERTED AFTER THE MAIN BODY OF THE TEXT
     %-------------------------------------------------------------------------

 

\end{document}                    % DO NOT DELETE THIS LINE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


